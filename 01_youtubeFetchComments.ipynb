{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df18f41-517f-4df3-b303-15cf8583e74f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm_ap_conn is already mounted\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "  dbutils.notebook.run(path = \"/Users/siddardha.kaja@aircanada.ca/DP-ETL Framework/prod/Generic-Mount\", arguments={'project_name': 'pm_ap_conn'}, timeout_seconds=28000)\n",
    "  print(\"pm_ap_conn is mounted\")\n",
    "except:\n",
    "  print(\"pm_ap_conn is already mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1657027d-d811-4d80-a5f3-4ced66cd50d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: oauth2client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b7212c21-3744-4af8-acdb-72785491ec79/lib/python3.10/site-packages (4.1.3)\nRequirement already satisfied: rsa>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from oauth2client) (4.9)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /databricks/python3/lib/python3.10/site-packages (from oauth2client) (0.2.8)\nRequirement already satisfied: pyasn1>=0.1.7 in /databricks/python3/lib/python3.10/site-packages (from oauth2client) (0.4.8)\nRequirement already satisfied: httplib2>=0.9.1 in /usr/lib/python3/dist-packages (from oauth2client) (0.20.2)\nRequirement already satisfied: six>=1.6.1 in /usr/lib/python3/dist-packages (from oauth2client) (1.16.0)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /databricks/python3/lib/python3.10/site-packages (from httplib2>=0.9.1->oauth2client) (3.0.9)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31e47c71-6588-4be4-ac6d-002fa3a5218d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: google-api-python-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b7212c21-3744-4af8-acdb-72785491ec79/lib/python3.10/site-packages (2.149.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /databricks/python3/lib/python3.10/site-packages (from google-api-python-client) (2.15.0)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b7212c21-3744-4af8-acdb-72785491ec79/lib/python3.10/site-packages (from google-api-python-client) (4.1.1)\nRequirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client) (0.20.2)\nRequirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-b7212c21-3744-4af8-acdb-72785491ec79/lib/python3.10/site-packages (from google-api-python-client) (0.2.0)\nRequirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /databricks/python3/lib/python3.10/site-packages (from google-api-python-client) (2.21.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.62.0)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /databricks/python3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.28.1)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /databricks/python3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.24.0)\nRequirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (1.26.14)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /databricks/python3/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.0.4)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fe98691-405e-47c3-8619-1193a988ef55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fetchYoutubeComments(**kwargs):\n",
    "  \"\"\"\n",
    "  The following params can be passed to the function to fetch the youtube video comments by searching them and the comments and their details are saved in a delta table:\n",
    "\n",
    "  keyword (required) : search keyword can pass multiple keywords by seperating them with `|`\n",
    "  apiVersion(required) : Its `v3` at the time of development\n",
    "  pathToServiceAccountJsonFile(required) : contact your manager for this\n",
    "  impersonationEmail(required) : contact your manager for this\n",
    "  locationDetails(optional) : pass lattitude, longitude and search radius in a list. Radius is in miles. Eg.,[lattitude, longitude, radius]\n",
    "  noResultsToFetch(optional) : depending on api request limit, pass the number else the default is 50. Should be in the mutiples of 50\n",
    "  searchOrder(optional) : The order parameter specifies the method that will be used to order resources in the API response. The default value is relevance. search order can 'date', 'relevance', 'rating', 'title', 'videoCount', 'viewCount'. \n",
    "  regionCode(optional) : The regionCode parameter instructs the API to return search results for videos that can be viewed in the specified country\n",
    "  languageCode(optional) : The relevanceLanguage parameter instructs the API to return search results that are most relevant to the specified language.\n",
    "  videoId(optional) : Declaring this param forces the function to fetch comments for this specific video\n",
    "  startDate(optional) : this indicates that the API response should only contain resources created at or after the specified date.\n",
    "  endDate(optional) : this indicates that the API response should only contain resources created before or at the specified date\n",
    "\n",
    "  usage:\n",
    "  fetchYoutubeComments(\n",
    "  keyword = \"air canada\",\n",
    "  apiVersion = 'v3',\n",
    "  noResultsToFetch = 100,\n",
    "  searchOrder = 'viewCount',\n",
    "  regionCode = 'CA',\n",
    "  startDate = '2023-12-01',\n",
    "  endDate = '2024-01-01',\n",
    "  impersonationEmail = 'XXXXXX-001@CCCCCC315001.iam.gserviceaccount.com',\n",
    "  pathToServiceAccountJsonFile = '/dbfs/mnt/Sandbox/XXX/XXX/XXXXXXXXXX.json'\n",
    "  )\n",
    "\n",
    "  The data will be deposited in the predetermined tables\n",
    "  \"\"\"\n",
    "  from typing import Optional, Tuple\n",
    "  import datetime\n",
    "  from pyspark.sql.functions import to_date, col\n",
    "  from pyspark.sql import SparkSession\n",
    "  spark = SparkSession.builder.getOrCreate()\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"31\")\n",
    "  spark.sql('SET spark.databricks.delta.retentionDurationCheck.enabled=false')\n",
    "  channel = 'youtube'\n",
    "  for key, value in kwargs.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "\n",
    "  def setConnection(apiVersion: str, channel: str, impersonationEmail: str, pathToServiceAccountJsonFile: str):\n",
    "    #creates service to youtube\n",
    "    from googleapiclient import discovery\n",
    "    from oauth2client.service_account import ServiceAccountCredentials\n",
    "    import httplib2\n",
    "\n",
    "    OAuthScopes = 'https://www.googleapis.com/auth/youtube.force-ssl'\n",
    "    impersonationEmail = impersonationEmail\n",
    "    pathToServiceAccountJsonFile = pathToServiceAccountJsonFile\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(pathToServiceAccountJsonFile,scopes=OAuthScopes)\n",
    "    if impersonationEmail:\n",
    "      credentials = credentials.create_delegated(impersonationEmail)\n",
    "    http = credentials.authorize(httplib2.Http())\n",
    "    service = discovery.build(channel, apiVersion, http=http)\n",
    "    return service\n",
    "  \n",
    "  def fetchcurrts(timezone, format):\n",
    "    #get current datetime from the mentioned timezone in the required format\n",
    "    import pytz\n",
    "    from datetime import datetime\n",
    "\n",
    "    est = pytz.timezone(timezone) \n",
    "\n",
    "    dt = datetime.now(est).strftime(format)\n",
    "    return dt\n",
    "\n",
    "  def takeFewVideos(li, liSize): \n",
    "    ## splits a list into multiple smaller size lists. we use to control the number of videoids for a request\n",
    "    for i in range(0, len(li), liSize):  \n",
    "        yield li[i:i + liSize] \n",
    "  \n",
    "  def getVideoIds(service, keyword, locationDetails, maxNoResults, searchOrder, regionCode, relevanceLanguage, publishBeginDate, publishEndDate):\n",
    "    #fetches the vidoeIds based on the search params\n",
    "\n",
    "    if locationDetails is not None:\n",
    "      createLocation = str(locationDetails[0])+','+str(locationDetails[1])\n",
    "      createRadius = str(locationDetails[2])+'mi'                                               \n",
    "    else:\n",
    "      createLocation = None    \n",
    "      createRadius =  None    \n",
    "\n",
    "    if publishBeginDate is not None:\n",
    "      publishBeginDate = publishBeginDate+'T00:00:00Z'\n",
    "\n",
    "    if publishEndDate is not None:\n",
    "      publishEndDate = publishEndDate+'T00:00:00Z'\n",
    "\n",
    "    request = service.search().list(\n",
    "          part=\"snippet\",\n",
    "          location=createLocation,\n",
    "          locationRadius=createRadius,\n",
    "          maxResults=maxNoResults,\n",
    "          order=searchOrder,\n",
    "          q=keyword,\n",
    "          type = 'video',\n",
    "          regionCode = regionCode,\n",
    "          relevanceLanguage = relevanceLanguage,     \n",
    "          publishedAfter=publishBeginDate,\n",
    "          publishedBefore=publishEndDate                                                                                      \n",
    "      )\n",
    "    response = request.execute()\n",
    "    nextPageToken = response['nextPageToken']\n",
    "    regionCode = response['regionCode']\n",
    "    fullList = []\n",
    "    for i in response['items']:\n",
    "      tempList = [i['id']['videoId'], spark.sql(\"select cast(from_utc_timestamp(current_timestamp, 'America/Toronto') as string)\").collect()[0][0],  regionCode, 'mainpage']\n",
    "      fullList.append(tempList)\n",
    "    return fullList, nextPageToken\n",
    "  \n",
    "  def getVideoIdsNextPage(service, fullList, nextPageToken, locationDetails, maxNoResults, searchOrder, regionCode, relevanceLanguage, publishBeginDate, publishEndDate):\n",
    "    #get vidoeIds for the next page as each page can hold only 50 items\n",
    "    if locationDetails is not None:\n",
    "      createLocation = str(locationDetails[0])+','+str(locationDetails[1])\n",
    "\n",
    "      createRadius = str(locationDetails[2])+'mi' \n",
    "                                                  \n",
    "    else:\n",
    "      createLocation = None    \n",
    "      createRadius =  None\n",
    "    if publishBeginDate is not None:\n",
    "      publishBeginDate = publishBeginDate+'T00:00:00Z'\n",
    "\n",
    "    if publishEndDate is not None:\n",
    "      publishEndDate = publishEndDate+'T00:00:00Z'\n",
    "\n",
    "    request = service.search().list(\n",
    "          part=\"snippet\",\n",
    "          location=createLocation,\n",
    "          locationRadius=createRadius,\n",
    "          maxResults=maxNoResults,\n",
    "          order=searchOrder,\n",
    "          pageToken=nextPageToken,\n",
    "          type = 'video',\n",
    "          regionCode = regionCode,\n",
    "          relevanceLanguage = relevanceLanguage,\n",
    "          publishedAfter=publishBeginDate,\n",
    "          publishedBefore=publishEndDate                                                                                    \n",
    "      )\n",
    "    response = request.execute()\n",
    "    \n",
    "    if 'nextPageToken' in response:\n",
    "      nextPageToken = response['nextPageToken']\n",
    "      regionCode = response['regionCode']\n",
    "      for i in response['items']:\n",
    "        tempList = [i['id']['videoId'], spark.sql(\"select cast(from_utc_timestamp(current_timestamp, 'America/Toronto') as string)\").collect()[0][0],  regionCode, nextPageToken]\n",
    "        fullList.append(tempList)\n",
    "    else:\n",
    "      nextPageToken = None\n",
    "    \n",
    "    return fullList, nextPageToken\n",
    "  \n",
    "  def getComments(service, videoId, commentSearchTerm):\n",
    "    # fetches the comments of a video\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "    from pyspark.sql.functions import lit\n",
    "    request = service.commentThreads().list(\n",
    "          part=\"snippet,replies\",\n",
    "          searchTerms=commentSearchTerm,\n",
    "          videoId=videoId,\n",
    "          maxResults = 100,\n",
    "          textFormat = 'plainText'\n",
    "      )\n",
    "    try:\n",
    "      response = request.execute()\n",
    "      commentsList = extractComments(response, videoId)\n",
    "    except:\n",
    "      response = [None]\n",
    "      commentsList = [[str(videoId), None, None, None, None, None, None, None, None, None, None, None, None, None, None]]\n",
    "  \n",
    "    if 'nextPageToken' in response:\n",
    "      nextPageToken = response['nextPageToken']\n",
    "      while nextPageToken != None:\n",
    "        res = getCommentsNextpage(service = service, commentSearchTerm = commentSearchTerm, videoId = videoId, nextPageToken = nextPageToken, commentsList = commentsList)\n",
    "        commentsList = res[0]\n",
    "        nextPageToken = res[1]\n",
    "    commentSchema = StructType([\n",
    "      StructField(\"videoId\", StringType(), False),\n",
    "      StructField(\"commentParentId\", StringType(), True),\n",
    "      StructField(\"commentKind\", StringType(), True),\n",
    "      StructField(\"commentEtag\", StringType(), True),\n",
    "      StructField(\"commentId\", StringType(), True),\n",
    "      StructField(\"comment\", StringType(), True),\n",
    "      StructField(\"commentAuthor\", StringType(), True),\n",
    "      StructField(\"commentAuthorChannelId\", StringType(), True),\n",
    "      StructField(\"commentCanRate\", StringType(), True),\n",
    "      StructField(\"commentViewerRating\", StringType(), True),\n",
    "      StructField(\"commentLikeCount\", StringType(), True),\n",
    "      StructField(\"commentReplyCount\", StringType(), True),\n",
    "      StructField(\"commentPublishedAt\", StringType(), True),\n",
    "      StructField(\"commentUpdatedAt\", StringType(), True),\n",
    "      StructField(\"commentModerationStatus\", StringType(), True)\n",
    "      ])\n",
    "    sparkDfComments = createSparkDf(commentsList, commentSchema)\n",
    "    sparkDfComments = sparkDfComments.withColumn('commentsFetchTimestampEst', lit( fetchcurrts('Canada/Eastern','%Y%m%d%H%M%S'))).withColumn('commentsUpdateTimestampEst', lit( fetchcurrts('Canada/Eastern','%Y%m%d%H%M%S')))\n",
    "\n",
    "    return sparkDfComments\n",
    "\n",
    "  def getCommentsNextpage(service, commentSearchTerm, videoId, nextPageToken, commentsList):\n",
    "    # each page can hold only 50 items, if there are more comments, this function fetches\n",
    "\n",
    "    request = service.commentThreads().list(\n",
    "        part=\"snippet,replies\",\n",
    "        searchTerms=commentSearchTerm,\n",
    "        videoId=videoId,\n",
    "        maxResults = 100,\n",
    "        textFormat = 'plainText',\n",
    "        pageToken = nextPageToken\n",
    "    )\n",
    "    try:\n",
    "      response = request.execute()\n",
    "      commentsListNextPage = extractComments(response, videoId)\n",
    "    except:\n",
    "      response = [None]\n",
    "      commentsListNextPage = [[str(videoId), None, None, None, None, None, None, None, None, None, None, None, None, None, None]]\n",
    "    commentsList.extend(commentsListNextPage)\n",
    "\n",
    "    if 'nextPageToken' in response:\n",
    "      nextPageToken = response['nextPageToken']\n",
    "    else:\n",
    "      nextPageToken = None\n",
    "    \n",
    "    return commentsList, nextPageToken\n",
    "\n",
    "  def createSparkDf(data, schema):\n",
    "    # creates pyspark df\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "  \n",
    "    df = spark.createDataFrame(data, schema)\n",
    "\n",
    "    return df\n",
    "  \n",
    "  def getVideoDetails(service, videoListStr):\n",
    "    # fetches video attributes\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType\n",
    "    request = service.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=videoListStr\n",
    "    )\n",
    "    videoDetails = request.execute()\n",
    "    videoDetailsList = []\n",
    "    pos = 0\n",
    "    for video in videoDetails['items']:\n",
    "      currentVideoList = []\n",
    "\n",
    "      try:\n",
    "        videoId = video['id']\n",
    "      except KeyError:\n",
    "        videoId = None\n",
    "      currentVideoList.insert(0, videoId)\n",
    "\n",
    "      try:\n",
    "        kind = video['kind']\n",
    "      except KeyError:\n",
    "        kind = None\n",
    "      currentVideoList.insert(1, kind)\n",
    "\n",
    "      try:\n",
    "        pubishedAt = video['snippet']['publishedAt']\n",
    "      except KeyError:\n",
    "        pubishedAt = None\n",
    "      currentVideoList.insert(2, pubishedAt)\n",
    "\n",
    "      try:\n",
    "        channelId = video['snippet']['channelId']\n",
    "      except KeyError:\n",
    "        channelId = None\n",
    "      currentVideoList.insert(3, channelId)\n",
    "\n",
    "      try:\n",
    "        channelTitle = video['snippet']['channelTitle']\n",
    "      except KeyError:\n",
    "        channelTitle = None\n",
    "      currentVideoList.insert(4, channelTitle)\n",
    "\n",
    "      try:\n",
    "        title = video['snippet']['title']\n",
    "      except KeyError:\n",
    "        title = None\n",
    "      currentVideoList.insert(5, title)\n",
    "\n",
    "      try:\n",
    "        description = video['snippet']['description']\n",
    "      except KeyError:\n",
    "        description = None\n",
    "      currentVideoList.insert(6, description)\n",
    "\n",
    "      try:\n",
    "        tags = video['snippet']['tags']\n",
    "      except KeyError:\n",
    "        tags = None\n",
    "      currentVideoList.insert(7, tags)\n",
    "\n",
    "      try:\n",
    "        duration = video['contentDetails']['duration']\n",
    "      except KeyError:\n",
    "        duration = None\n",
    "      currentVideoList.insert(8, duration)\n",
    "\n",
    "      try:\n",
    "        regionRestriction = video['contentDetails']['regionRestriction']\n",
    "      except KeyError:\n",
    "        regionRestriction = None\n",
    "      currentVideoList.insert(9, regionRestriction)\n",
    "\n",
    "      try:\n",
    "        viewCount = video['statistics']['viewCount']\n",
    "      except KeyError:\n",
    "        viewCount = None\n",
    "      currentVideoList.insert(10, viewCount)\n",
    "\n",
    "      try:\n",
    "        likeCount = video['statistics']['likeCount']\n",
    "      except KeyError:\n",
    "        likeCount = None\n",
    "      currentVideoList.insert(11, likeCount)\n",
    "\n",
    "      try:\n",
    "        favoriteCount = video['statistics']['favoriteCount']\n",
    "      except KeyError:\n",
    "        favoriteCount = None\n",
    "      currentVideoList.insert(12, favoriteCount)\n",
    "\n",
    "      try:\n",
    "        commentCount = video['statistics']['commentCount']\n",
    "      except KeyError:\n",
    "        commentCount = None\n",
    "      currentVideoList.insert(13, commentCount)\n",
    "\n",
    "      videoDetailsList.insert(pos, currentVideoList)\n",
    "      pos = pos+1\n",
    "\n",
    "    videoAttrSchema = StructType([\n",
    "          StructField(\"videoId\", StringType(), True),\n",
    "          StructField(\"kind\", StringType(), True),\n",
    "          StructField(\"publishedAt\", StringType(), True),\n",
    "          StructField(\"channelId\", StringType(), True),\n",
    "          StructField(\"channelTitle\", StringType(), True),\n",
    "          StructField(\"title\", StringType(), True),\n",
    "          StructField(\"description\", StringType(), True),\n",
    "          StructField(\"tags\", StringType(), True),\n",
    "          StructField(\"duration\", StringType(), True),\n",
    "          StructField(\"regionRestriction\", StringType(), True),\n",
    "          StructField(\"viewCount\", StringType(), True),\n",
    "          StructField(\"likeCount\", StringType(), True),\n",
    "          StructField(\"favoriteCount\", StringType(), True),\n",
    "          StructField(\"commentCount\", StringType(), True)\n",
    "      ])\n",
    "    sparkDfVideoAttr = createSparkDf(videoDetailsList, videoAttrSchema)\n",
    "    \n",
    "    return sparkDfVideoAttr\n",
    "  \n",
    "  def extractComments(comm, videoId):\n",
    "    # extracts comments from the response object\n",
    "    commentList = []\n",
    "    commentPos = 0\n",
    "\n",
    "    replyList = []\n",
    "    replyPos = 0\n",
    "\n",
    "    for comment in comm['items']:\n",
    "      eachComment = []\n",
    "      pos = 0\n",
    "\n",
    "      videoId = videoId\n",
    "      eachComment.insert(pos, videoId)\n",
    "      pos += 1\n",
    "\n",
    "      commentParentId = None\n",
    "      eachComment.insert(pos, commentParentId)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        kind = comment['snippet']['topLevelComment']['kind']\n",
    "      except KeyError:\n",
    "        kind = None\n",
    "      eachComment.insert(pos, kind)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        etag = comment['snippet']['topLevelComment']['etag']\n",
    "      except Exception as e:\n",
    "        etag = None\n",
    "      eachComment.insert(pos, etag)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        commentid = comment['snippet']['topLevelComment']['id']\n",
    "      except KeyError:\n",
    "        commentid = None\n",
    "      eachComment.insert(pos, commentid)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        textDisplay = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "      except KeyError:\n",
    "        textDisplay = None\n",
    "      eachComment.insert(pos, textDisplay)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        authorDisplayName =  comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "      except KeyError:\n",
    "        authorDisplayName = None\n",
    "      eachComment.insert(pos, authorDisplayName)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        authorChannelId =  comment['snippet']['topLevelComment']['snippet']['authorChannelId']['value']\n",
    "      except KeyError:\n",
    "        authorChannelId = None\n",
    "      eachComment.insert(pos, authorChannelId)\n",
    "      pos += 1\n",
    "\n",
    "\n",
    "      try:\n",
    "        canRate = comment['snippet']['topLevelComment']['snippet']['canRate']\n",
    "      except KeyError:\n",
    "        canRate = None\n",
    "      eachComment.insert(pos, canRate)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        viewerRating = comment['snippet']['topLevelComment']['snippet']['viewerRating']\n",
    "      except KeyError:\n",
    "        viewerRating = None\n",
    "      eachComment.insert(pos, viewerRating)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        likeCount = comment['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "      except KeyError:\n",
    "        likeCount = None\n",
    "      eachComment.insert(pos, likeCount)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        totalReplyCount = comment['snippet']['totalReplyCount']\n",
    "      except KeyError:\n",
    "        totalReplyCount = None\n",
    "\n",
    "      if totalReplyCount != 0 and 'replies' in comment.keys():\n",
    "        for reply in comment['replies']['comments']:\n",
    "          eachReply = []\n",
    "          eachReplyPos = 0\n",
    "\n",
    "          try:\n",
    "            replyvideoId = reply['snippet']['videoId']\n",
    "          except KeyError:\n",
    "            replyvideoId = None\n",
    "          eachReply.insert(eachReplyPos, replyvideoId)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            commentParentId = reply['snippet']['parentId']\n",
    "          except KeyError:\n",
    "            commentParentId = None\n",
    "          eachReply.insert(eachReplyPos, commentParentId)\n",
    "          eachReplyPos += 1\n",
    "          \n",
    "          replyKind = 'youtube#reply'\n",
    "          eachReply.insert(eachReplyPos, replyKind)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyEtag = reply['etag']\n",
    "          except KeyError:\n",
    "            replyEtag = None\n",
    "          eachReply.insert(eachReplyPos, replyEtag)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyId = reply['id']\n",
    "          except KeyError:\n",
    "            replyId = None\n",
    "          eachReply.insert(eachReplyPos, replyId)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyTextDisplay = reply['snippet']['textDisplay']\n",
    "          except KeyError:\n",
    "            replyTextDisplay = None\n",
    "          eachReply.insert(eachReplyPos, replyTextDisplay)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyAuthorName = reply['snippet']['authorDisplayName']\n",
    "          except KeyError:\n",
    "            replyAuthorName = None\n",
    "          eachReply.insert(eachReplyPos, replyAuthorName)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyChannelId = reply['snippet']['channelId']\n",
    "          except KeyError:\n",
    "            replyChannelId = None\n",
    "          eachReply.insert(eachReplyPos, replyChannelId)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyCanRate = reply['snippet']['canRate']\n",
    "          except KeyError:\n",
    "            replyCanRate = None\n",
    "          eachReply.insert(eachReplyPos, replyCanRate)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyViewerRating = reply['snippet']['viewerRating']\n",
    "          except KeyError:\n",
    "            replyViewerRating = None\n",
    "          eachReply.insert(eachReplyPos, replyViewerRating)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "\n",
    "          try:\n",
    "            replyLikeCount = reply['snippet']['likeCount']\n",
    "          except KeyError:\n",
    "            replyLikeCount = None\n",
    "          eachReply.insert(eachReplyPos, replyLikeCount)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          replyReplyCount = None\n",
    "          eachReply.insert(eachReplyPos, replyReplyCount)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyPublishedAt = reply['snippet']['publishedAt']\n",
    "          except KeyError:\n",
    "            replyPublishedAt = None\n",
    "          eachReply.insert(eachReplyPos, replyPublishedAt)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            replyUpdatedAt = reply['snippet']['updatedAt']\n",
    "          except KeyError:\n",
    "            replyUpdatedAt = None\n",
    "          eachReply.insert(eachReplyPos, replyUpdatedAt)\n",
    "          eachReplyPos += 1\n",
    "\n",
    "          try:\n",
    "            moderationStatus =  reply['snippet']['moderationStatus']\n",
    "          except KeyError:\n",
    "            moderationStatus = None\n",
    "          eachReply.insert(eachReplyPos, moderationStatus)\n",
    "\n",
    "          replyList.insert(replyPos, eachReply)\n",
    "          replyPos = replyPos + 1\n",
    "          \n",
    "      eachComment.insert(pos, totalReplyCount)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        publishedAt =  comment['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "      except KeyError:\n",
    "        publishedAt = None\n",
    "      eachComment.insert(pos, publishedAt)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        updatedAt = comment['snippet']['topLevelComment']['snippet']['updatedAt']\n",
    "      except KeyError:\n",
    "        updatedAt = None\n",
    "      eachComment.insert(pos, updatedAt)\n",
    "      pos += 1\n",
    "\n",
    "      try:\n",
    "        moderationStatus =  comment['snippet']['topLevelComment']['snippet']['moderationStatus']\n",
    "      except KeyError:\n",
    "        moderationStatus = None\n",
    "      eachComment.insert(pos, moderationStatus)\n",
    "      commentList.insert(commentPos, eachComment)\n",
    "      commentPos = commentPos+1\n",
    "      totalCommentsAndReplies = commentList + replyList\n",
    "\n",
    "    return totalCommentsAndReplies\n",
    "  \n",
    "  def filterFurther(keyword):\n",
    "    #sometimes search can return irrelevant videos, so this function filters out the unnecessary videos by making sure that either title or description or tags should contain the key work atleast once\n",
    "    keywordList = keyword.split(\"|\")\n",
    "    keywordFilterString = ''\n",
    "    concatString = 'or'\n",
    "    for i in range(0, len(keywordList)):\n",
    "      #convert to string and strip for whitespaces at beginning and ending\n",
    "      keywordItem = str(keywordList[i]).strip()\n",
    "      if i == (len(keywordList)-1):\n",
    "        tempString = \"lower(title) like '%\"+keywordItem.lower()+\"%' or lower(description) like '%\"+keywordItem.lower()+\"%' or lower(tags) like '%\"+keywordItem.lower()+\"%'\"\n",
    "        keywordFilterString = keywordFilterString+tempString\n",
    "      else:\n",
    "        tempString = \"lower(title) like '%\"+keywordItem.lower()+\"%' or lower(description) like '%\"+keywordItem.lower()+\"%' or lower(tags) like '%\"+keywordItem.lower()+\"%' or \"\n",
    "        keywordFilterString = keywordFilterString+tempString\n",
    "    return keywordFilterString\n",
    "\n",
    "  def main(channel: str ,keyword: str,  apiVersion: str, pathToServiceAccountJsonFile: str, impersonationEmail: str, commentSearchTermsList : Optional = None, locationDetails: Optional[Tuple[str, str, int]] = None, noResultsToFetch: Optional[int] = 50, searchOrder: Optional[str] = None, regionCode: Optional[str] = None, languageCode: Optional[str] = None, videoId: Optional[str] = None, startDate: Optional[str] = None, endDate: Optional[str] = None  ):\n",
    "  \n",
    "    #parameters validation\n",
    "    #validate keyword param\n",
    "\n",
    "    commentsMountPath = \"/mnt/Sandbox/pm_ap_conn/sid_test/youtube/daily/youtubeVideoCommentsReplies\"\n",
    "    videosMountPath = \"/mnt/Sandbox/pm_ap_conn/sid_test/youtube/daily/youtubeVideoAttributes\"\n",
    "    assert isinstance(keyword, str), \"keyword needs to be in string\"\n",
    "\n",
    "    \n",
    "    assert isinstance(channel, str), \"channel must be in string format\"\n",
    "    assert channel in ['youtube'], \"searchOrder should be among `youtube`\"\n",
    "    \n",
    "\n",
    "    #validate api_version param\n",
    "    assert isinstance(apiVersion, str), \"keyword needs to be in string\"\n",
    "    #validate location_details param\n",
    "    if locationDetails:\n",
    "      if isinstance(locationDetails, tuple):\n",
    "        assert len(locationDetails) == 3, \"locationDetails expecting 3 values longitue, lattitude and range in miles \"\n",
    "        assert isinstance(locationDetails[0], str ), \"lattitude must be in string format\"\n",
    "        assert isinstance(locationDetails[1], str ), \"longitude must be in string format\"\n",
    "        assert isinstance(locationDetails[2], int ), \"Radius must be in integer format\"\n",
    "      else:\n",
    "        raise Exception(\"locationDetails needs to be a tuple\")\n",
    "\n",
    "    #validate max_no_results param\n",
    "    if noResultsToFetch:\n",
    "      assert isinstance(noResultsToFetch, int), \"noResultsToFetch must be in integer format\"\n",
    "\n",
    "    #validate search_order param\n",
    "    if searchOrder:\n",
    "      assert isinstance(searchOrder, str), \"searchOrder must be in string format\"\n",
    "      assert searchOrder in ['date', 'relevance', 'rating', 'title', 'videoCount', 'viewCount'], \"searchOrder should be among `date`, `rating`, `relevance`, `title`, `videoCount`, `viewCount`. Default value is `date`\"\n",
    "\n",
    "    #validate region_code param\n",
    "    if regionCode: \n",
    "      assert isinstance(regionCode, str), \"regionCode must be in string format\"\n",
    "      assert len(regionCode) == 2, \"regionCode should be an ISO 3166-1 alpha-2 country code. Please refer to https://www.iso.org/obp/ui/#search\"\n",
    "\n",
    "    #validate language_code param\n",
    "    if languageCode: \n",
    "      assert isinstance(languageCode, str), \"languageCode must be in string format\"\n",
    "      assert len(languageCode) != 2, \"languageCode should be an ISO 639-1 two-letter language code. Please refer to https://www.loc.gov/standards/iso639-2/php/code_list.php\"\n",
    "    \n",
    "    assert isinstance(impersonationEmail, str), \"impersonationEmail needs to be in string\"\n",
    "\n",
    "    assert isinstance(pathToServiceAccountJsonFile, str), \"pathToServiceAccountJsonFile needs to be in string\"\n",
    "\n",
    "    #validate video_id param\n",
    "    if videoId:\n",
    "      assert isinstance(videoId, str), \"videoId must be in string format\"\n",
    "    \n",
    "    dateFormat = \"%Y-%m-%d\"\n",
    "    if startDate:\n",
    "      try:\n",
    "        datetime.datetime.strptime(startDate, dateFormat)\n",
    "      except ValueError:\n",
    "        print(\"Incorrect startDate Format. It should be 'YYYY-MM-DD'\")\n",
    "        raise\n",
    "        return\n",
    "      except:\n",
    "        raise\n",
    "        return\n",
    "    \n",
    "    if endDate:\n",
    "      try:\n",
    "        datetime.datetime.strptime(endDate, dateFormat)\n",
    "      except ValueError:\n",
    "        print(\"Incorrect endDate Format. It should be 'YYYY-MM-DD'\")\n",
    "        raise\n",
    "        return\n",
    "      except:\n",
    "        raise\n",
    "        return\n",
    "    \n",
    "\n",
    "\n",
    "    if channel == 'youtube':\n",
    "      service  = setConnection(apiVersion = apiVersion, channel = channel, impersonationEmail = impersonationEmail, pathToServiceAccountJsonFile = pathToServiceAccountJsonFile)\n",
    "      \n",
    "\n",
    "      if not videoId:\n",
    "        firstPageVidoes = getVideoIds(service, keyword = keyword, locationDetails = locationDetails, maxNoResults= '50', searchOrder = searchOrder, regionCode = regionCode, relevanceLanguage = languageCode, publishBeginDate = startDate, publishEndDate = endDate)\n",
    "        \n",
    "        if firstPageVidoes[1]:\n",
    "          nextPageToken = firstPageVidoes[1]\n",
    "\n",
    "          fullList = firstPageVidoes[0]\n",
    "          videoCount = len(firstPageVidoes[0])\n",
    "          while videoCount < noResultsToFetch:\n",
    "            b = getVideoIdsNextPage(service, fullList = fullList, nextPageToken = nextPageToken, locationDetails = locationDetails, maxNoResults= '50', searchOrder = searchOrder, regionCode = regionCode, relevanceLanguage = languageCode, publishBeginDate = startDate, publishEndDate = endDate)\n",
    "            try:\n",
    "              nextPageToken = b[1]\n",
    "              fullList = b[0]\n",
    "            except:\n",
    "              break\n",
    "            videoCount = len(b[0])\n",
    "\n",
    "      else:\n",
    "        # if the individual video id is passed, we will not fetch the video search result, we will only engage with provided videoId\n",
    "        if 'regionCode' in locals():\n",
    "          fullList = [[videoId, spark.sql(\"select cast(from_utc_timestamp(current_timestamp, 'America/Toronto') as string)\").collect()[0][0], regionCode, 'mainpage']] \n",
    "        else:\n",
    "          fullList = [[videoId, spark.sql(\"select cast(from_utc_timestamp(current_timestamp, 'America/Toronto') as string)\").collect()[0][0],  'None', \"mainpage\"]] \n",
    "\n",
    "      videosDf = createSparkDf(fullList, schema = [\"videoId\", \"fetchTimestampEst\", \"regionCode\", \"pageDetails\"])\n",
    "\n",
    "      #we fetch the video details of 25 videos at a time, the request can fetch up to 50 videos at a time but i limited it to 25 to avoid an unknown error. ##[{'message': 'The request specifies an invalid filter parameter.', 'domain': 'youtube.parameter', 'reason': 'invalidFilters', 'location': 'parameters.', 'locationType': 'other'}]##\n",
    "      videoIdChunks = list(takeFewVideos(fullList, 25)) \n",
    "      for i in range(len(videoIdChunks)):\n",
    "        videoIdChunk = videoIdChunks[i]\n",
    "        videoList = []\n",
    "        for j in videoIdChunk:\n",
    "          videoList.append(j[0])\n",
    "\n",
    "        videoListStr = str(videoList)[1:-1]\n",
    "        videoListStr= videoListStr.replace(\"'\", \"\")\n",
    "        videoListStr = videoListStr.replace(\" \", \"\")\n",
    "\n",
    "        if i == 0:\n",
    "          currentFetchVideoAttrDf = getVideoDetails(service, videoListStr)\n",
    "        else:\n",
    "          tempFetchVideoAttrDf = getVideoDetails(service, videoListStr)\n",
    "          currentFetchVideoAttrDf = currentFetchVideoAttrDf.union(tempFetchVideoAttrDf)\n",
    "          \n",
    "      # we will join the initial df with fetched video ids using the search params and dataframe with video details for each of the fetched videoids. At the same time, publishedDate col is derived and if any duplicated records present, they are dropped. we are futher filtering the fetched videos by their title, description and tags to match the keywords passed by the user\n",
    "      currentFetchVideoAttrDf = currentFetchVideoAttrDf.join(videosDf, currentFetchVideoAttrDf.videoId == videosDf.videoId, \"fullouter\").select(currentFetchVideoAttrDf[\"*\"],to_date(col(\"publishedAt\")).alias(\"publishedDate\"), col(\"regionCode\"),col(\"fetchTimestampEst\")).filter(filterFurther(keyword)).dropDuplicates(['videoId'])\n",
    "      \n",
    "      #create the temp view to merge with the existing data, fetchTimestampEst will not be updated if the videoId is present in the new data\n",
    "      currentFetchVideoAttrDf.createOrReplaceTempView(\"tempvideodetails\")\n",
    "\n",
    "      #update old common records\n",
    "      spark.sql(\"\"\"MERGE into delta.`{0}` as old USING tempvideodetails as new on new.videoId = old.videoId and (new.title <> old.title or new.description <> old.description or new.tags <> old.tags)  WHEN MATCHED THEN UPDATE SET  old.effectiveToDateEst = cast(from_utc_timestamp(current_timestamp, 'America/Toronto') as string), old.activeFlag = 'N'\"\"\".format(videosMountPath))\n",
    "\n",
    "      ##insert common new records\n",
    "      spark.sql(\"\"\"insert into delta.`{0}` (videoId, kind, publishedAt, channelId, channelTitle, title, description, tags, duration, regionRestriction, viewCount, likeCount, favoriteCount, commentCount, publishedDate, regionCode, effectiveFromDateEst, effectiveToDateEst, activeFlag ) select new.videoId, new.kind, new.publishedAt, new.channelId, new.channelTitle, new.title, new.description, new.tags, new.duration, new.regionRestriction, new.viewCount, new.likeCount, new.favoriteCount, new.commentCount, new.publishedDate, new.regionCode, fetchTimestampEst as effectiveFromDateEst, '9999-12-31 99:99:99.999', 'Y' from tempvideodetails  as new INNER JOIN delta.`{0}` as old on new.videoId = old.videoId and (new.title <> old.title or new.description <> old.description or new.tags <> old.tags)\"\"\".format(videosMountPath))   \n",
    "\n",
    "      ##insert new records\n",
    "      spark.sql(\"\"\"insert into delta.`{0}` (videoId, kind, publishedAt, channelId, channelTitle, title, description, tags, duration, regionRestriction, viewCount, likeCount, favoriteCount, commentCount, publishedDate, regionCode, effectiveFromDateEst, effectiveToDateEst, activeFlag ) select new.videoId, new.kind, new.publishedAt, new.channelId, new.channelTitle, new.title, new.description, new.tags, new.duration, new.regionRestriction, new.viewCount, new.likeCount, new.favoriteCount, new.commentCount, new.publishedDate, new.regionCode, fetchTimestampEst as effectiveFromDateEst, '9999-12-31 99:99:99.999', 'Y' from tempvideodetails  as new LEFT JOIN delta.`{0}` as old on new.videoId = old.videoId WHERE old.videoId IS NULL\"\"\".format(videosMountPath))\n",
    "\n",
    "\n",
    "      youtubeVideoAttributesMain = spark.read.format(\"delta\").load(videosMountPath)\n",
    "\n",
    "      #we collect the videoIds whose comment counts are not 0. Need the videoId as python list\n",
    "      CommentedVideoList = youtubeVideoAttributesMain.select(youtubeVideoAttributesMain.videoId).where(youtubeVideoAttributesMain.commentCount != '0').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "      for commentSearchTerm in commentSearchTermsList:\n",
    "        for videoPos in range(len(CommentedVideoList)):\n",
    "          videoId = CommentedVideoList[videoPos]\n",
    "          if videoPos == 0:\n",
    "            VideoComments = getComments(service, videoId, commentSearchTerm)\n",
    "          else:\n",
    "            nonFirstVideoComments = getComments(service, videoId, commentSearchTerm)\n",
    "            VideoComments = VideoComments.union(nonFirstVideoComments)\n",
    "        #create the temp view to merge with the existing data, commentFetchTimestampEst will not be updated if the videoId is present in the new data, commentPublishedDate is also created, duplicated commentIds are dropped\n",
    "        VideoComments = VideoComments.select(VideoComments[\"*\"],to_date(col(\"commentUpdatedAt\")).alias(\"commentUpdatedDate\")).dropDuplicates(['commentId']).dropna(subset=\"commentId\").createOrReplaceTempView(\"tempcomments\")\n",
    "        \n",
    "        \n",
    "        spark.sql(\"\"\"merge into delta.`{0}` as original using tempcomments as new on (original.commentId = new.commentId) when matched then update set commentParentId= new.commentParentId, commentKind= new.commentKind, commentEtag= new.commentEtag,  comment= new.comment, commentAuthor= new.commentAuthor, commentAuthorChannelId= new.commentAuthorChannelId, commentCanRate= new.commentCanRate, commentViewerRating= new.commentViewerRating, commentLikeCount= new.commentLikeCount, commentReplyCount= new.commentReplyCount, commentPublishedAt= new.commentPublishedAt, commentUpdatedAt= new.commentUpdatedAt, commentModerationStatus= new.commentModerationStatus, commentUpdatedDate = new.commentUpdatedDate,  commentsUpdateTimestampEst= new.commentsUpdateTimestampEst when not matched then insert (videoId, commentParentId, commentKind, commentEtag, commentId, comment, commentAuthor, commentAuthorChannelId, commentCanRate, commentViewerRating, commentLikeCount, commentReplyCount, commentPublishedAt, commentUpdatedAt, commentModerationStatus, commentUpdatedDate, commentsFetchTimestampEst, commentsUpdateTimestampEst) values (new.videoId, new.commentParentId, new.commentKind, new.commentEtag, new.commentId, new.comment, new.commentAuthor, new.commentAuthorChannelId, new.commentCanRate, new.commentViewerRating, new.commentLikeCount, new.commentReplyCount, new.commentPublishedAt, new.commentUpdatedAt, new.commentModerationStatus, new.commentUpdatedDate, new.commentsFetchTimestampEst, new.commentsUpdateTimestampEst) \"\"\". format(commentsMountPath)) \n",
    "\n",
    "        spark.sql('''optimize delta.`{0}`'''.format(commentsMountPath))\n",
    "        spark.sql('''optimize delta.`{0}`'''.format(videosMountPath))\n",
    "        spark.sql('''vacuum delta.`{0}` retain 0 hours'''.format(commentsMountPath))\n",
    "        spark.sql('''vacuum delta.`{0}` retain 0 hours'''.format(videosMountPath))\n",
    "    \n",
    "    return VideoComments\n",
    "  # (\"successfully fetched comments\")\n",
    "\n",
    "\n",
    "  main(channel = channel, keyword = keyword, commentSearchTermsList = commentSearchTermsList, pathToServiceAccountJsonFile = pathToServiceAccountJsonFile, impersonationEmail = impersonationEmail, apiVersion = apiVersion, noResultsToFetch = noResultsToFetch, searchOrder = searchOrder, regionCode = regionCode, startDate = startDate, endDate = endDate)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2880afe8-880a-4850-9422-7be81fa8cef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
     ]
    }
   ],
   "source": [
    "a = fetchYoutubeComments(keyword = \"air canada|aircanada|aeroplan\", apiVersion = 'v3', noResultsToFetch = 5, commentSearchTermsList = [\"air canada\"], searchOrder = 'viewCount', regionCode = 'CA', startDate = '2024-10-20', endDate = '2024-10-25', impersonationEmail = 'sid-testing-001@avacado-315001.iam.gserviceaccount.com', pathToServiceAccountJsonFile = \"/dbfs/mnt/Sandbox/pm_ap_conn/sid_test/avacado_315001_8a8bb7a78015.json\")\n",
    "display(a)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_youtubeFetchComments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
