{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "143cc306-e016-406d-9473-e1783d0e4c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def commentSentimentAnalysis(pysparkDf, colName):\n",
    "  \"\"\"\n",
    "  This functions performs the sentiment analysis on the passed pyspark dataframe. We use cardiffnlp/twitter-xlm-roberta-base-sentiment model from huggingface for the scoring. Passed pyspark dataframe should have a column called `comment` and if the schema of the passed dataframe dont match, the functions returns the result else the result is stored in the predetermined location as a delta table. The expected dataframe should have the these columns title:string, description:string, tags:string, videoId:string, commentId:string, commentUpdatedDate:date, commentAuthorChannelId:string, comment:string\n",
    "\n",
    "  usage:\n",
    "  \n",
    "  commentSentimentAnalysis(pysparkDf = youtubeVideoCommentsRepliesFiltered)\n",
    "  \n",
    "  \"\"\"\n",
    "  from pyspark.sql import SparkSession\n",
    "  from pyspark.sql.functions import col, udf\n",
    "  from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField\n",
    "  from pyspark.storagelevel import StorageLevel\n",
    "  from pyspark.sql import SparkSession\n",
    "  spark = SparkSession.builder.getOrCreate()\n",
    "  \n",
    "  #destination path\n",
    "  sentimentsMountPath = \"/mnt/Sandbox/pm_ap_conn/sid_test/youtube/commentSentimentsTest202401060630\"\n",
    "  #tweetnlp function load\n",
    "  def getSentimentModel():\n",
    "    import tweetnlp\n",
    "    model = tweetnlp.load_model('sentiment', multilingual=True)\n",
    "    return model\n",
    "  \n",
    "  def sentimentUdf(model, text):\n",
    "    # Perform sentiment analysis on the entire text\n",
    "    x = model.sentiment(text, return_probability=True, batch_size=32)\n",
    "    label = x['label']\n",
    "    probability = x['probability'][label]\n",
    "    sentiment_details = {\"label\": label, \"probability\": probability}\n",
    "    return sentiment_details\n",
    "\n",
    "  def main(pysparkDf, colName):\n",
    "    #load the model only once\n",
    "    sentimentModel = getSentimentModel()\n",
    "    #read input dataset\n",
    "    feed = pysparkDf.repartition(31)\n",
    "    #define UDF for sentiment analysis\n",
    "    sentimentSchema = MapType(StringType(), StringType())\n",
    "    sentimentSparkUdf = udf(lambda comments: sentimentUdf(sentimentModel, comments), sentimentSchema)\n",
    "\n",
    "    #apply the UDF to the spark dataFrame\n",
    "    result = feed.withColumn(\"sentimentAnalysis\", sentimentSparkUdf(col(colName))) \\\n",
    "                        .withColumn(\"sentimentCategory\", col(\"sentimentAnalysis\").getItem(\"label\")) \\\n",
    "                        .withColumn(\"sentimentCategoryProbability\", col(\"sentimentAnalysis\").getItem(\"probability\"))\n",
    "    #save the result as delta format in datalake\n",
    "    try:\n",
    "      result.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"False\").save(sentimentsMountPath)\n",
    "    except:\n",
    "      print(\"schema of the passed df dont meet the requirements. Hence returned!!\")\n",
    "    return result\n",
    "\n",
    "  main(pysparkDf, colName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11afa6fa-f6ec-4df7-8764-4d130604e433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting tensorflow==2.10\n  Using cached tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\nCollecting keras<2.11,>=2.10.0\n  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\nRequirement already satisfied: wrapt>=1.11.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (1.14.1)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (0.4.0)\nRequirement already satisfied: libclang>=13.0.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (15.0.6.1)\nCollecting keras-preprocessing>=1.1.1\n  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nRequirement already satisfied: numpy>=1.20 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (1.23.5)\nCollecting protobuf<3.20,>=3.9.2\n  Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (65.6.3)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (1.48.2)\nRequirement already satisfied: absl-py>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (1.0.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (0.2.0)\nRequirement already satisfied: termcolor>=1.1.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (2.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (1.6.3)\nRequirement already satisfied: typing-extensions>=3.6.6 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (4.4.0)\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.10) (1.16.0)\nCollecting tensorflow-estimator<2.11,>=2.10.0\n  Using cached tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (0.35.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (3.3.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (23.2)\nCollecting tensorboard<2.11,>=2.10\n  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\nRequirement already satisfied: h5py>=2.9.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (3.7.0)\nRequirement already satisfied: flatbuffers>=2.0 in /databricks/python3/lib/python3.10/site-packages (from tensorflow==2.10) (23.5.26)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /databricks/python3/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.10) (0.38.4)\nRequirement already satisfied: markdown>=2.6.8 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (3.4.1)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nRequirement already satisfied: werkzeug>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.2.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.28.1)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /databricks/python3/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow==2.10) (2.21.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (4.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (5.3.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.2.8)\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.26.14)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (1.3.1)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow==2.10) (2022.12.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /databricks/python3/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (2.1.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow==2.10) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow==2.10) (3.2.0)\nInstalling collected packages: tensorboard-plugin-wit, keras, tensorflow-estimator, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: keras\n    Found existing installation: keras 2.14.0\n    Not uninstalling keras at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c9779a9-f217-48e9-aa2b-c6fb645ada45\n    Can't uninstall 'keras'. No files were found to uninstall.\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.14.0\n    Not uninstalling tensorflow-estimator at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c9779a9-f217-48e9-aa2b-c6fb645ada45\n    Can't uninstall 'tensorflow-estimator'. No files were found to uninstall.\n  Attempting uninstall: tensorboard-data-server\n    Found existing installation: tensorboard-data-server 0.7.2\n    Not uninstalling tensorboard-data-server at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c9779a9-f217-48e9-aa2b-c6fb645ada45\n    Can't uninstall 'tensorboard-data-server'. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.24.0\n    Not uninstalling protobuf at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c9779a9-f217-48e9-aa2b-c6fb645ada45\n    Can't uninstall 'protobuf'. No files were found to uninstall.\n  Attempting uninstall: google-auth-oauthlib\n    Found existing installation: google-auth-oauthlib 1.0.0\n    Not uninstalling google-auth-oauthlib at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c9779a9-f217-48e9-aa2b-c6fb645ada45\n    Can't uninstall 'google-auth-oauthlib'. No files were found to uninstall.\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.14.1\n    Not uninstalling tensorboard at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2c9779a9-f217-48e9-aa2b-c6fb645ada45\n    Can't uninstall 'tensorboard'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-feature-engineering 0.2.1 requires pyspark<4,>=3.1.2, which is not installed.\ntensorflow-cpu 2.14.1 requires keras<2.15,>=2.14.0, but you have keras 2.10.0 which is incompatible.\ntensorflow-cpu 2.14.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\ntensorflow-cpu 2.14.1 requires tensorboard<2.15,>=2.14, but you have tensorboard 2.10.1 which is incompatible.\ntensorflow-cpu 2.14.1 requires tensorflow-estimator<2.15,>=2.14.0, but you have tensorflow-estimator 2.10.0 which is incompatible.\nfacets-overview 1.1.1 requires protobuf>=3.20.0, but you have protobuf 3.19.6 which is incompatible.\nSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.10.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install -U tensorflow==2.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3738c092-53ab-423a-8b3e-406aee768034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting tweetnlp==0.4.4\n  Downloading tweetnlp-0.4.4.tar.gz (54 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 kB 2.5 MB/s eta 0:00:00\n  Preparing metadata (setup.py): started\n  Preparing metadata (setup.py): finished with status 'done'\nCollecting ray[tune]\n  Downloading ray-2.38.0-cp310-cp310-manylinux2014_x86_64.whl (66.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.0/66.0 MB 13.3 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from tweetnlp==0.4.4) (1.23.5)\nCollecting urlextract\n  Downloading urlextract-1.9.0-py3-none-any.whl (21 kB)\nCollecting transformers<=4.21.2\n  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 143.7 MB/s eta 0:00:00\nCollecting huggingface-hub<=0.9.1\n  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 120.7/120.7 kB 26.2 MB/s eta 0:00:00\nRequirement already satisfied: sentence_transformers in /databricks/python3/lib/python3.10/site-packages (from tweetnlp==0.4.4) (2.2.2)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.10/site-packages (from tweetnlp==0.4.4) (2.0.1+cpu)\nRequirement already satisfied: datasets in /databricks/python3/lib/python3.10/site-packages (from tweetnlp==0.4.4) (2.15.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<=0.9.1->tweetnlp==0.4.4) (23.2)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<=0.9.1->tweetnlp==0.4.4) (6.0)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<=0.9.1->tweetnlp==0.4.4) (4.64.1)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<=0.9.1->tweetnlp==0.4.4) (2.28.1)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<=0.9.1->tweetnlp==0.4.4) (3.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<=0.9.1->tweetnlp==0.4.4) (4.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers<=4.21.2->tweetnlp==0.4.4) (2022.7.9)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 143.6 MB/s eta 0:00:00\nRequirement already satisfied: xxhash in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (3.4.1)\nRequirement already satisfied: aiohttp in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (3.9.1)\nRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (2023.6.0)\nRequirement already satisfied: multiprocess in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (0.70.14)\nRequirement already satisfied: pyarrow>=8.0.0 in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (8.0.0)\nRequirement already satisfied: pyarrow-hotfix in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (0.5)\nCollecting datasets\n  Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 472.7/472.7 kB 66.0 MB/s eta 0:00:00\nCollecting pyarrow>=15.0.0\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 39.9/39.9 MB 24.2 MB/s eta 0:00:00\nCollecting requests\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 12.4 MB/s eta 0:00:00\nCollecting tqdm\n  Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.4/78.4 kB 14.5 MB/s eta 0:00:00\nCollecting datasets\n  Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 73.4 MB/s eta 0:00:00\n  Downloading datasets-3.0.0-py3-none-any.whl (474 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.3/474.3 kB 72.8 MB/s eta 0:00:00\n  Downloading datasets-2.21.0-py3-none-any.whl (527 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 527.3/527.3 kB 73.9 MB/s eta 0:00:00\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (0.3.6)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (1.5.3)\n  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 547.8/547.8 kB 51.8 MB/s eta 0:00:00\n  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.1/542.1 kB 63.1 MB/s eta 0:00:00\n  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 71.7 MB/s eta 0:00:00\n  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 49.1 MB/s eta 0:00:00\n  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 20.5 MB/s eta 0:00:00\n  Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.7/536.7 kB 41.9 MB/s eta 0:00:00\n  Downloading datasets-2.17.0-py3-none-any.whl (536 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.6/536.6 kB 69.5 MB/s eta 0:00:00\n  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 507.1/507.1 kB 58.2 MB/s eta 0:00:00\n  Downloading datasets-2.16.0-py3-none-any.whl (507 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 507.1/507.1 kB 55.6 MB/s eta 0:00:00\n  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 520.4/520.4 kB 58.0 MB/s eta 0:00:00\n  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 65.8 MB/s eta 0:00:00\n  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 44.5 MB/s eta 0:00:00\n  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.3/519.3 kB 47.9 MB/s eta 0:00:00\n  Downloading datasets-2.14.3-py3-none-any.whl (519 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.1/519.1 kB 53.5 MB/s eta 0:00:00\n  Downloading datasets-2.14.2-py3-none-any.whl (518 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 518.9/518.9 kB 70.4 MB/s eta 0:00:00\n  Downloading datasets-2.14.1-py3-none-any.whl (492 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 492.4/492.4 kB 39.6 MB/s eta 0:00:00\n  Downloading datasets-2.14.0-py3-none-any.whl (492 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 492.2/492.2 kB 71.1 MB/s eta 0:00:00\n  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 512.7/512.7 kB 64.4 MB/s eta 0:00:00\n  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 486.2/486.2 kB 73.8 MB/s eta 0:00:00\n  Downloading datasets-2.13.0-py3-none-any.whl (485 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.6/485.6 kB 65.4 MB/s eta 0:00:00\n  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 474.6/474.6 kB 76.7 MB/s eta 0:00:00\n  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 468.7/468.7 kB 43.5 MB/s eta 0:00:00\n  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 65.4 MB/s eta 0:00:00\nRequirement already satisfied: responses<0.19 in /databricks/python3/lib/python3.10/site-packages (from datasets->tweetnlp==0.4.4) (0.18.0)\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from ray[tune]->tweetnlp==0.4.4) (8.0.4)\nRequirement already satisfied: frozenlist in /databricks/python3/lib/python3.10/site-packages (from ray[tune]->tweetnlp==0.4.4) (1.4.1)\nRequirement already satisfied: jsonschema in /databricks/python3/lib/python3.10/site-packages (from ray[tune]->tweetnlp==0.4.4) (4.17.3)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /databricks/python3/lib/python3.10/site-packages (from ray[tune]->tweetnlp==0.4.4) (4.24.0)\nRequirement already satisfied: aiosignal in /databricks/python3/lib/python3.10/site-packages (from ray[tune]->tweetnlp==0.4.4) (1.3.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from ray[tune]->tweetnlp==0.4.4) (1.0.7)\nCollecting tensorboardX>=1.9\n  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 17.0 MB/s eta 0:00:00\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers->tweetnlp==0.4.4) (1.1.1)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers->tweetnlp==0.4.4) (0.15.2+cpu)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers->tweetnlp==0.4.4) (1.10.0)\nRequirement already satisfied: sentencepiece in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers->tweetnlp==0.4.4) (0.1.99)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from sentence_transformers->tweetnlp==0.4.4) (3.7)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch->tweetnlp==0.4.4) (1.11.1)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch->tweetnlp==0.4.4) (3.1.2)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch->tweetnlp==0.4.4) (2.8.4)\nRequirement already satisfied: platformdirs in /databricks/python3/lib/python3.10/site-packages (from urlextract->tweetnlp==0.4.4) (2.5.2)\nCollecting uritools\n  Downloading uritools-4.0.3-py3-none-any.whl (10 kB)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from urlextract->tweetnlp==0.4.4) (3.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets->tweetnlp==0.4.4) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets->tweetnlp==0.4.4) (1.9.4)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets->tweetnlp==0.4.4) (22.1.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp->datasets->tweetnlp==0.4.4) (4.0.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->huggingface-hub<=0.9.1->tweetnlp==0.4.4) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->huggingface-hub<=0.9.1->tweetnlp==0.4.4) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->huggingface-hub<=0.9.1->tweetnlp==0.4.4) (1.26.14)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch->tweetnlp==0.4.4) (2.1.1)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /databricks/python3/lib/python3.10/site-packages (from jsonschema->ray[tune]->tweetnlp==0.4.4) (0.18.0)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence_transformers->tweetnlp==0.4.4) (1.2.0)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->datasets->tweetnlp==0.4.4) (2022.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas->datasets->tweetnlp==0.4.4) (2.8.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn->sentence_transformers->tweetnlp==0.4.4) (2.2.0)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch->tweetnlp==0.4.4) (1.2.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.10/site-packages (from torchvision->sentence_transformers->tweetnlp==0.4.4) (9.4.0)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->tweetnlp==0.4.4) (1.16.0)\nBuilding wheels for collected packages: tweetnlp\n  Building wheel for tweetnlp (setup.py): started\n  Building wheel for tweetnlp (setup.py): finished with status 'done'\n  Created wheel for tweetnlp: filename=tweetnlp-0.4.4-py3-none-any.whl size=40457 sha256=fce9894d5dbd5ad6567f856c7a9372b0e341ac700d7ebc130587e65aa4a8f4bb\n  Stored in directory: /root/.cache/pip/wheels/0b/bc/68/5f63d35830b187cefdf1907a052ec18b85bfdc67bb9562c735\nSuccessfully built tweetnlp\nInstalling collected packages: tokenizers, uritools, tensorboardX, urlextract, ray, huggingface-hub, transformers, datasets, tweetnlp\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-968aa554-8367-40d2-9789-3fd6e4ec7057\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.19.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-968aa554-8367-40d2-9789-3fd6e4ec7057\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.1\n    Not uninstalling transformers at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-968aa554-8367-40d2-9789-3fd6e4ec7057\n    Can't uninstall 'transformers'. No files were found to uninstall.\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.15.0\n    Not uninstalling datasets at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-968aa554-8367-40d2-9789-3fd6e4ec7057\n    Can't uninstall 'datasets'. No files were found to uninstall.\nSuccessfully installed datasets-2.10.1 huggingface-hub-0.9.1 ray-2.38.0 tensorboardX-2.6.2.2 tokenizers-0.12.1 transformers-4.21.2 tweetnlp-0.4.4 uritools-4.0.3 urlextract-1.9.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install tweetnlp==0.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6286e2bf-c29f-48a1-8697-0428463989f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1330541243429584>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m display(\u001B[43mcommentSentimentAnalysis\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2024-08-01\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2024-08-10\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslang_dict_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/Workspace/Users/siddardha.kaja@aircanada.ca/EnglishSDK/slangDict.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m<command-1330541243429580>, line 135\u001B[0m, in \u001B[0;36mcommentSentimentAnalysis\u001B[0;34m(startCommentDate, endCommentDate, slang_dict_path)\u001B[0m\n",
       "\u001B[1;32m    133\u001B[0m   finalDf \u001B[38;5;241m=\u001B[39m commentSentimentAnalysis(forSentiAnalysis, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpandedText\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    134\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m finalDf\n",
       "\u001B[0;32m--> 135\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstartCommentDate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendCommentDate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslang_dict_path\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-1330541243429580>, line 133\u001B[0m, in \u001B[0;36mcommentSentimentAnalysis.<locals>.main\u001B[0;34m(startCommentDate, endCommentDate, slang_dict_path)\u001B[0m\n",
       "\u001B[1;32m    130\u001B[0m forSentiAnalysis \u001B[38;5;241m=\u001B[39m cleanedTextDf\u001B[38;5;241m.\u001B[39munion(langFilteredNonEN_1)\n",
       "\u001B[1;32m    132\u001B[0m \u001B[38;5;66;03m#do sentiment analysis\u001B[39;00m\n",
       "\u001B[0;32m--> 133\u001B[0m finalDf \u001B[38;5;241m=\u001B[39m \u001B[43mcommentSentimentAnalysis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mforSentiAnalysis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexpandedText\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m finalDf\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: commentSentimentAnalysis() missing 1 required positional argument: 'slang_dict_path'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "commentSentimentAnalysis() missing 1 required positional argument: 'slang_dict_path'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: commentSentimentAnalysis() missing 1 required positional argument: 'slang_dict_path'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-1330541243429584>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m display(\u001B[43mcommentSentimentAnalysis\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2024-08-01\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m2024-08-10\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslang_dict_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m/Workspace/Users/siddardha.kaja@aircanada.ca/EnglishSDK/slangDict.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n",
        "File \u001B[0;32m<command-1330541243429580>, line 135\u001B[0m, in \u001B[0;36mcommentSentimentAnalysis\u001B[0;34m(startCommentDate, endCommentDate, slang_dict_path)\u001B[0m\n\u001B[1;32m    133\u001B[0m   finalDf \u001B[38;5;241m=\u001B[39m commentSentimentAnalysis(forSentiAnalysis, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpandedText\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    134\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m finalDf\n\u001B[0;32m--> 135\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstartCommentDate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mendCommentDate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslang_dict_path\u001B[49m\u001B[43m)\u001B[49m\n",
        "File \u001B[0;32m<command-1330541243429580>, line 133\u001B[0m, in \u001B[0;36mcommentSentimentAnalysis.<locals>.main\u001B[0;34m(startCommentDate, endCommentDate, slang_dict_path)\u001B[0m\n\u001B[1;32m    130\u001B[0m forSentiAnalysis \u001B[38;5;241m=\u001B[39m cleanedTextDf\u001B[38;5;241m.\u001B[39munion(langFilteredNonEN_1)\n\u001B[1;32m    132\u001B[0m \u001B[38;5;66;03m#do sentiment analysis\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m finalDf \u001B[38;5;241m=\u001B[39m \u001B[43mcommentSentimentAnalysis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mforSentiAnalysis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexpandedText\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m finalDf\n",
        "\u001B[0;31mTypeError\u001B[0m: commentSentimentAnalysis() missing 1 required positional argument: 'slang_dict_path'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(commentSentimentAnalysis('2024-08-01', '2024-08-10', slang_dict_path = '/Workspace/Users/siddardha.kaja@aircanada.ca/EnglishSDK/slangDict.txt'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 649951959723982,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_commentSentimentAnalysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
