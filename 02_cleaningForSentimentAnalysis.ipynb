{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97a131e-a179-4fdc-bfb1-ffe88ad570d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm_ap_conn is already mounted\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "  dbutils.notebook.run(path = \"/Users/siddardha.kaja@aircanada.ca/DP-ETL Framework/prod/Generic-Mount\", arguments={'project_name': 'pm_ap_conn'}, timeout_seconds=28000)\n",
    "  print(\"pm_ap_conn is mounted\")\n",
    "except:\n",
    "  print(\"pm_ap_conn is already mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7424c8-7e16-4012-aed0-d2b3bd56c6e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: sentencepiece in /databricks/python3/lib/python3.10/site-packages (0.1.99)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86eeb52a-88db-4eed-8cbd-3c81735622ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: contractions==0.1.73 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6f59833e-26b4-409c-a34a-e2e93147ff39/lib/python3.10/site-packages (0.1.73)\nRequirement already satisfied: textsearch>=0.0.21 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6f59833e-26b4-409c-a34a-e2e93147ff39/lib/python3.10/site-packages (from contractions==0.1.73) (0.0.24)\nRequirement already satisfied: pyahocorasick in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6f59833e-26b4-409c-a34a-e2e93147ff39/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions==0.1.73) (2.1.0)\nRequirement already satisfied: anyascii in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6f59833e-26b4-409c-a34a-e2e93147ff39/lib/python3.10/site-packages (from textsearch>=0.0.21->contractions==0.1.73) (0.3.2)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install contractions==0.1.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1196534a-6ac1-46e2-8fac-7a3762677f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: protobuf==5.28.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-6f59833e-26b4-409c-a34a-e2e93147ff39/lib/python3.10/site-packages (5.28.3)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==5.28.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "162bd0d6-f509-49a9-b6b6-9e56eed9be73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "startDate = '2024-10-20'\n",
    "endDate = '2024-10-25'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bbcba26-534a-4b3d-9f15-7018b1ed8112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cleaningForSentimentAnalysis(startDate, endDate):\n",
    "  from transformers import pipeline\n",
    "  from pyspark.sql.types import StringType, MapType, FloatType\n",
    "  from pyspark.sql.functions import udf, col, lit, create_map, lower\n",
    "  from pyspark import StorageLevel\n",
    "  spark.conf.set(\"spark.sql.shuffle.partitions\", \"31\")\n",
    "\n",
    " \n",
    "\n",
    "  def spellingCorrectionModel():\n",
    "    modelName = \"oliverguhr/spelling-correction-english-base\"\n",
    "    model = pipeline(\"text2text-generation\",model= modelName, tokenizer = modelName)\n",
    "    return model\n",
    "\n",
    "  def commentSpellingCorrection(model, text):\n",
    "    correctedSpelling = model(text, max_length = len(text)+1)\n",
    "    return correctedSpelling[0]\n",
    "\n",
    "  def spellingCorrection(pysparkDf, colName):\n",
    "    spellingCorrectionModelFunc = spellingCorrectionModel()\n",
    "    feed = pysparkDf.repartition(31)\n",
    "    spellingCorrectionSchema = MapType(StringType(), StringType())\n",
    "    spellingCorrectionUdf = udf(lambda comments: commentSpellingCorrection(spellingCorrectionModelFunc, comments), spellingCorrectionSchema)\n",
    "    result = feed.withColumn(\"correctedSpelling\", spellingCorrectionUdf(col(colName))).withColumn(\"correctedSpellingText\", col(\"correctedSpelling\").getItem(\"generated_text\"))\n",
    "    return result\n",
    "\n",
    "  def detectLanguageModel():\n",
    "    modelName = \"papluca/xlm-roberta-base-language-detection\"\n",
    "    translate = pipeline(\"text-classification\", model=modelName, tokenizer = modelName, max_length=512, truncation=True)\n",
    "    return translate\n",
    "\n",
    "  def commentLanguageDetection(model, text):\n",
    "    commentLanguage = model(text)\n",
    "    return commentLanguage[0]\n",
    "\n",
    "  def languageDetection(pysparkDf, colName):\n",
    "    languageModel = detectLanguageModel()\n",
    "    feed = pysparkDf.repartition(31)\n",
    "    langSchema = MapType(StringType(), StringType())\n",
    "    langSparkUdf = udf(lambda comments: commentLanguageDetection(languageModel, comments), langSchema)\n",
    "    result = feed.withColumn(\"langDetection\", langSparkUdf(col(colName))).withColumn(\"detectedLang\", col(\"langDetection\").getItem(\"label\")).withColumn(\"detectedLangProbability\", col(\"langDetection\").getItem(\"score\"))\n",
    "    return result\n",
    "\n",
    "  def preprocess(df):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField\n",
    "    # remove the records whose comments are null\n",
    "    dfRemoveNull = df.filter(df.comment != \"\")\n",
    "    preprocess_udf = F.udf(lambda text: \" \".join(['@user' if t.startswith('@') and len(t) > 1 else '@user' if t.startswith('â€‹@') and len(t) > 1 else 'http' if t.startswith('http') else t for t in text.split(\" \")]), StringType())\n",
    "    processedDf = dfRemoveNull.withColumn(\"processedComment\", preprocess_udf(dfRemoveNull[\"comment\"]))\n",
    "    return processedDf\n",
    "    \n",
    "  def cleanText(pysparkDf, slangDict):\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import udf, col\n",
    "    from pyspark.sql.types import StringType\n",
    "    import contractions\n",
    "\n",
    "    def preCleanText(inputText, reverse):\n",
    "      if reverse == False:\n",
    "        inputText1 = inputText.replace(',', ' , ')\n",
    "        inputText2 = inputText1.replace('!', ' ! ')\n",
    "        inputText3 = inputText2.replace('.', ' . ')\n",
    "      elif reverse == True:\n",
    "        inputText1 = inputText.replace(' , ', ',')\n",
    "        inputText2 = inputText1.replace(' ! ', '!')\n",
    "        inputText3 = inputText2.replace(' . ', '.')\n",
    "      return inputText3\n",
    "    def expandText(text, slangDict):\n",
    "      inputText = preCleanText(text, reverse = False)\n",
    "      words = inputText.split()\n",
    "      expandedText = ' '.join(slangDict.get(word.lower(), word).upper() if word.isupper() else slangDict.get(word.lower(), word) for word in words)\n",
    "      outputText = preCleanText(expandedText, reverse = True)\n",
    "      #apply contractions\n",
    "      fixText = str(contractions.fix(outputText))\n",
    "      return fixText\n",
    "\n",
    "    \n",
    "    def main(pysparkDf, slangDict):\n",
    "      expandTextUDF = udf(lambda text: expandText(text, slangDict), StringType())\n",
    "\n",
    "      result = pysparkDf.withColumn(\"expandedText\", expandTextUDF(col(\"processedComment\")))\n",
    "      # result.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"True\").save(sentimentsMountPath)\n",
    "      return result\n",
    "    return main(pysparkDf, slangDict)\n",
    "  \n",
    "  def main():\n",
    "    import ast\n",
    "    with open('/Workspace/Users/siddardha.kaja@aircanada.ca/EnglishSDK/slangDict.txt') as f:\n",
    "      slang_dict_str = f.read()\n",
    "\n",
    "    slangDict = ast.literal_eval(slang_dict_str)\n",
    "\n",
    "    youtubeVideoAttributesMain = spark.read.format(\"delta\").load(\"/mnt/Sandbox/pm_ap_conn/sid_test/youtube/youtubeVideoAttributes\")\n",
    "\n",
    "    youtubeVideoCommentsReplies = spark.read.format(\"delta\").load(\"/mnt/Sandbox/pm_ap_conn/sid_test/youtube/youtubeVideoCommentsReplies\")\n",
    "\n",
    "    sampleSize = 0.001\n",
    "    commentDateRange = ('2024-10-01', '2024-10-25')\n",
    "    youtubeVideoCommentsRepliesFiltered = youtubeVideoCommentsReplies.join(youtubeVideoAttributesMain,youtubeVideoCommentsReplies.videoId ==  youtubeVideoAttributesMain.videoId,\"inner\").select( youtubeVideoAttributesMain[\"title\"], youtubeVideoAttributesMain[\"description\"], youtubeVideoAttributesMain[\"tags\"], youtubeVideoCommentsReplies[\"videoId\"], youtubeVideoCommentsReplies[\"commentId\"], youtubeVideoCommentsReplies[\"commentUpdatedDate\"], youtubeVideoCommentsReplies[\"commentAuthorChannelId\"], youtubeVideoCommentsReplies[\"comment\"]).filter((youtubeVideoAttributesMain.activeFlag == \"Y\") & (youtubeVideoCommentsReplies.commentUpdatedDate.between(*commentDateRange)) ).sample(sampleSize)\n",
    "\n",
    "\n",
    "    # <____________________________>\n",
    "    youtubeVideoCommentsRepliesFiltered = youtubeVideoCommentsRepliesFiltered.repartition(31)\n",
    "    # preprocess to remove the usernames and urls from the comments\n",
    "    preprocessedDf = preprocess(youtubeVideoCommentsRepliesFiltered)\n",
    "\n",
    "    #pass preprocessed df to get the language detected\n",
    "    langDetected = languageDetection(preprocessedDf, \"comment\")\n",
    "    langDetected.cache()\n",
    "    # #filter en comments and filter them into a new df\n",
    "    langFilteredEN = langDetected.filter(langDetected.detectedLang == \"en\")\n",
    "\n",
    "    #clean the text by expanding slags and contractions\n",
    "    cleanedTextDf = cleanText(langFilteredEN, slangDict)\n",
    "\n",
    "    # # correct spellings\n",
    "    # correctedSpellingsDf = spellingCorrection(cleanedTextDf, \"expandedText\")\n",
    "\n",
    "    # scoop the records that are not en into another df\n",
    "    langFilteredNonEN = langDetected.filter(langDetected.detectedLang != \"en\")\n",
    "    # create cols to match the en df for further steps\n",
    "    # langFilteredNonEN_1 = langFilteredNonEN.withColumn('expandedText', lit(\"None\")).withColumn('correctedSpelling', create_map(lit(\"None\"), lit(\"None\"))).withColumn('correctedSpellingText', col(\"comment\"))\n",
    "    langFilteredNonEN_1 = langFilteredNonEN.withColumn('expandedText', lit(\"comment\"))\n",
    "    # # combine the en and non-en dfs \n",
    "    forSentiAnalysis = cleanedTextDf.union(langFilteredNonEN_1)\n",
    "      \n",
    "    # #do sentiment analysis\n",
    "    # finalDf = commentSentimentAnalysis(forSentiAnalysis, \"expandedText\")\n",
    "    return(youtubeVideoCommentsRepliesFiltered)\n",
    " \n",
    "  return main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f19575ad-ed10-4346-9523-88d42ccfd39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>description</th><th>tags</th><th>videoId</th><th>commentId</th><th>commentUpdatedDate</th><th>commentAuthorChannelId</th><th>comment</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tags",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "videoId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "commentId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "commentUpdatedDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "commentAuthorChannelId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessed = cleaningForSentimentAnalysis(startDate, endDate)\n",
    "display(preprocessed)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_cleaningForSentimentAnalysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
